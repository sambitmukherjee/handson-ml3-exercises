{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Things to note:**\n",
        "\n",
        "1. The task is binary classification. However, the target variable comprises values that are either -1 or 1 (not 0 or 1). Which activation function is applied to the final layer when the target is either -1 or 1? (It certainly isn't the sign activation function mentioned in Chapter 10. This is because its derivative is 0, and hence it doesn't work with gradient descent.) Also, which loss function works in this case? Notice that there is only 1 output unit.\n",
        "2. So far, you've thought of each hidden neuron / convolution filter as producing a new feature. For example, for predicting housing prices, the combination of '*contains playground*' & '*proximity to nearest school*' could be a newly discovered hidden feature '*suitability for children*'. This is a correct viewpoint. But the hidden neurons in the TensorFlow Playground provide an additional perspective. Each neuron is actually separating the original input space (defined by the `x_1` and `x_2` axes). In other words, each neuron is generating a decision boundary! When you choose the ReLU activation function, each hidden neuron is dividing the original input space into two parts - zero and positive. (The color coding is showing the following: which combination of values of `x_1` and `x_2` lead to a zero activation value, and which combination of values of `x_1` and `x_2` lead to a positive activation value. And the border of the zero space and the positive space is the decision boundary.) And neurons in a subsequent layer are able to divide the input space in an increasingly sophisticated manner.\n",
        "3. As noted above, neurons in a subsequent layer have more sophisticated (non-linear) decision boundaries. However, they have linear decision boundaries vis-a-vis its own inputs (i.e., the outputs of the previous layer). They are non-linear vis-a-vis inputs that are at least two layers back (e.g., the original inputs to the network).\n",
        "4. In the input layer, `x_1**2`, `x_2**2`, `x_1 * x_2`, `sin(x_1)` and `sin(x_2)` also produce decision boundaries (by dividing the input space into positive and negative regions)! This kind of connects the dots between the \"*each hidden neuron discovers a new feature*\" perspective and the \"*each hidden neuron divides the original input space*\" perspective. From the `x_1**2`, `x_2**2`, `x_1 * x_2`, `sin(x_1)` and `sin(x_2)` features in the input layer, we see that any kind of feature engineering divides the original input space. So what the hidden neurons are doing is nothing but automating the process of feature engineering (which ends up separating the input space).\n",
        "5. The '*Noise*' slider can be used to control the quantity of noisy (incorrect) labels. It's not to be confused with noisy features."
      ],
      "metadata": {
        "id": "AtITjvpO2QzS"
      },
      "id": "AtITjvpO2QzS"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFqgS6aCF3oP"
      },
      "id": "gFqgS6aCF3oP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}